‚úçWhat are Data Structures?
- A way to store, organize & manage information(or data) in a way that allows you
the programmer to easily access or modify the values within them.
- Provide backbone to the programs
- Store information & access & manipulate information effectively
- Basic Data structures like => Password & online directories,etc
- Advanced Data structures Like => Autocomplete of text messages, undo, redo functions, etc
- Efficiency => the metrics used to judge the speed & Efficiency of different data structures
 
‚úçSeries Overview
üíª Arrays
üíª ArrayLists
üíª Stacks
üíª Queues
üíª LinkedLists
üíª Doubly-LinkedLists
üíª Dictionaries
üíª Hash-Tables
üíª Trees
üíª Tries
üíª Heaps
üíª Graphs

üíª (06:55) Measuring Efficiency with BigO Notation
- Quantifiable way to measure to how efficient certain data structures are at different tasks we might as of it 
 - Searching through
 - Modifying
 - Access
- The industry standard for this kind of measurement => BigO Notation
- BigO Notation => Based on four criteria
  - Accessing elements
  - Searching elements
  - Inserting elements
  - Deleting elements 
‚å®Ô∏è (09:45) Time Complexity Equations
   - Works by Inserting the size of the data-set as an integer n, and returning the number of operations that need to be conducted by the computer before the function can finish
   - The number of operations that need to be conducted by the computer before completion of that function
   - We always use the worst-case scenario when judging these data structures
‚å®Ô∏è (11:13) The Meaning of BigO
   - The syntax for the time complexity equations included a BigO and then a set of parenthesis
     - The parenthesis houses the function
     - A random function and its time complexity is measured based on the data sets, our integer n
     - n => number of operations
     - More data set => More instructions
‚å®Ô∏è (12:42) Why BigO?
     - We measure Efficiency in # of operations performed because measuring by how long the function takes to run would be silly => Measuring by time is biased towards better hardware
‚å®Ô∏è (13:18) Quick Recap
    - Measure Efficiency based on 4 metrics => Accessing, Searching, Inserting, Deleting
    - Modeled by an equation which takes in size of data-set n => number of operations needed to perform the task
‚å®Ô∏è (14:27) Types of Time Complexity Equations
    - O(1) =>Constant algorithm, absolute best, no matter what the size of your data set is , the task will be completed in a single instruction
    - O(log n) => Logarithmic algorithm, Next fastest type of time complexity, fast completion time, more efficient
    - O(n) => Linear algorithm Common time complexity, the last of the decent equations
    - O(n log n) => Relatively Bad
    - O(n2)=> Polynomial algorithm, Very bad in terms of Efficiency 
    - O(2n)=> Exponential algorithm, Very bad in terms of Efficiency
‚å®Ô∏è (19:42) Final Note on Time Complexity Equations
    - Time complexity equations are NOT the only metric you should be using the gauge which data structure to use 
    - Some provide other functionality that make them extremely useful
    
üíª (20:21) The Array
    - A pretty common data structure taught in most programming classes 

‚å®Ô∏è (20:58) Array Basics
    - fundamentally a list of similar values
    - can be store anything => usernames, high scores, prices, etc.
    - store values of the same data type => integer, float ,etc.
    - item referred to as "element"
    - three attributes => A name , A Type , A size 

‚å®Ô∏è (22:09) Array Names
    - Array Names => name of the array, used to reference & interact with it
      Example: Names =  ["John Smith", "Gary Vee"]; Salaries = [10000, 12400]

‚å®Ô∏è (22:59) Parallel Arrays
    - Two or more arrays which => contain the same no. of elements => have corresponding values in the same position
    - extremely useful for storing different types of data about the same entity

‚å®Ô∏è (23:59) Array Types
    - what type of information is stored or will be stored within that array
    - has to hold all the same type of information

‚å®Ô∏è (24:30) Array Size
    - Immutable 
    - a set integer that is fixed upon creation of the array
    - represents the total amount of elements that are able to be stored within the array

‚å®Ô∏è (25:45) Creating Arrays
    - 2 different ways to create an array in most languages
     => populate the array with elements right then & there
     => set a specific size for the array, then populate it later

‚å®Ô∏è (26:11) Populate-First Arrays
    - defining & filling an array as soon as you create it is used mainly for when you already know which values are going to be held within it
    - The way varies from languages => Java, Python, C#
     => int array[] = {1,2,3}; //Java 
     => array =[1,2,3]         // Python
     => int[] array = {1,2,3}; // C#

‚å®Ô∏è (28:09) Populate-Later Arrays
    - creating an array by setting an initial size for our array, but not filing it with any elements
     => slowly populate it as the programs run
     => used for user-entered information
     => int array [] =new int[10]; // Java
     => int[] arrray = new int[10]; // C#

‚å®Ô∏è (30:22) Numerical Indexes
    - get information that is stored withing the array, we use a number Indexes
    - an integer which corresponds to an element within the array

‚å®Ô∏è (31:57) Replacing information in an Array
    - Referencing an arrays index is also h0w we replace elements within an array

‚å®Ô∏è (32:42) 2-Dimensional Arrays
    - an array with an array at each index is known as a 2-Dimensional array
    - useful for programming chessboard, bingo board, image with rgb values,etc
    - includes 2 indexes => one for the column and other with row
    - there are 3d arrays and 4d arrays for complex programming

‚å®Ô∏è (35:01) Arrays as a Data Structure
    - accessing, searching, inserting, deleting in an array
    - accessing => O(1) that is why arrays are fixed size
    - searching => O(n) because of unsorted lists, must use linear search
    - inserting => O(n) because inserting an element within the array requires you to shift every element that's after the index you want to insert the value at to the right one space
    - deleting => O(n) because deleting an element within the array requires you to shift every element to the right of the one you want to delete down one index

‚å®Ô∏è (42:21) Pros and Cons
    - Pros => Good for storing similar contiguous data, O(1) accessing power, very basic, easy to learn and master
    - Cons => size of the array cannot be changed once Initialized,
    inserting and deleting are not efficient, can be wasting storage space
    - Overall, pretty reliable, has some flaws as well as advantages, Can be used in almost any program if need be, but sometimes you may want extra functionality

üíª (43:33) The ArrayList
    - can be thought as growing array
    - size is dynamic

‚å®Ô∏è (44:42) Structure of the ArrayList
    - backed by an array
    - makes it have a similar functionality

‚å®Ô∏è (45:19) Initializing an ArrayList
    - varies based on languages
    - Python list are both array and ArrayLists
    - ArrayList<integer> arrayList = new ArrayList<integer>(); //Java
    - ArrayList arrayList = new ArrayList(); //C#

‚å®Ô∏è (47:34) ArrayList Functionality
    - can be thought as evolved array
    - beefier, more functionality, more powerful
    - pre-build arrayList "class" => prebuilt functions that are at our disposal
    - varies based on languages

‚å®Ô∏è (49:30) ArrayList Methods
    - because of the variability, only 6 common methods
    
‚å®Ô∏è (50:26) Add Method
    - two different types: 
    - Add(Object)=> appends the element you pass in as an argument to the end of the arrayList => Autoboxing, puts the object to next open indexes
    - Add(Object, Index) => appends the element you pass in as an argument at the index you pass in

‚å®Ô∏è (53:57) Remove Method
    - two methods:
    - Remove(Index) => removes the object at the index you provide
    - Remove(Object) => removes the first instance of the object passed into the arrayList

‚å®Ô∏è (55:33) Get Method
    - Get(Index) => returns the object contained at the given index

‚å®Ô∏è (55:59) Set Method
    - set method is how we replace elements within the arrayList
    - Set (Index, Object) => sets the element at the index which you passed in, to the object you also passed in

‚å®Ô∏è (56:57) Clear Method
    - Clear() => clears the arrayList, deleting every element entirely

‚å®Ô∏è (57:30) toArray Method
    - used to convert an arrayList to an Array
    - toArray() => converts the arrayList to an array. Must be equal to the creation of an new array

‚å®Ô∏è (59:00) ArrayList as a Data Structure
    - accessing, searching, inserting, deleting
    - accessing => get method, O(1), 
    - searching => O(n)
    - inserting => Add(Object,Index); Add(Object), O(n)
    - deleting => Remove(Object),Remove(Index),O(n)

‚å®Ô∏è (1:03:12) Comparing and Contrasting with Arrays
    - Arrays- Fixed size, can store all data types, methods to be created,
    don't requires much memory and upkeep,
      smaller tasks, where you won't be interacting or changing the data
    - ArrayLists- Dynamic size, can only store objects, methods are created for you, requires more memory use and upkeep
      more interactive programs where you will be Modifying the data

üíª (1:05:02) The Stack
‚å®Ô∏è (1:05:06) The Different types of Data Structures
‚å®Ô∏è (1:05:51) Random Access Data Structures
    - Provides O(1) Accessing
    - Independent elements
    - Arrays and ArrayLists

‚å®Ô∏è (1:06:10) Sequential Access Data Structures
    - can only be accessed in a particular order
    - Each element is dependent on the others
    - May only be obtainable through those those other elements
    - also limited access 
    - Do not provide O(1) accessing
    - Stacks, Queues, Linked lists

‚å®Ô∏è (1:07:36) Stack Basics
    - We add elements and remove elements according to the LIFO principle
    - Last in First Out

‚å®Ô∏è (1:09:01) Common Stack Methods
    - Push Method
    - Pop Method
    - Peek Method
    - Contains Method

‚å®Ô∏è (1:09:45) Push Method
    - Pushes an object or element onto the group of top of the Stack
    - Push(object)
    - exampleStach.push("something")

‚å®Ô∏è (1:10:32) Pop Method
    - removes an element from the top of the Stack
    - Pop()

‚å®Ô∏è (1:11:46) Peek Method
    - Allows you to get the value at the top of the list without removing it
    - Peek()

‚å®Ô∏è (1:12:27) Contains Method
    - Used for searching through the Stack
    - contains()

‚å®Ô∏è (1:13:23) Time Complexity Equations
    - accessing: O(n)
    - searching: O(n)
    - inserting: O(1), push(object)
    - deleting: O(1) pop()

‚å®Ô∏è (1:15:28) Uses for Stacks
    - are used everywhere, both in the actual writing of code as well as in the real-world situations
    - Recursion=> The process of functions repeatedly calling themselves
    - Undo/Redo, Back-programming
    
üíª (1:18:01) The Queue
‚å®Ô∏è (1:18:51) Queue Basics
‚å®Ô∏è (1:20:44) Common Queue Methods
‚å®Ô∏è (1:21:13) Enqueue Method
‚å®Ô∏è (1:22:20) Dequeue Method
‚å®Ô∏è (1:23:08) Peek Method
‚å®Ô∏è (1:24:15) Contains Method
‚å®Ô∏è (1:25:05) Time Complexity Equations
‚å®Ô∏è (1:27:05) Common Queue Uses
üíª (1:28:16) The Linked List
‚å®Ô∏è (1:31:37) LinkedList Visualization
‚å®Ô∏è (1:33:55) Adding and Removing Information
‚å®Ô∏è (1:41:28) Time Complexity Equations
‚å®Ô∏è (1:44:26) Uses for LinkedLists
üíª (1:47:19) The Doubly-LinkedList
‚å®Ô∏è (1:48:44) Visualization
‚å®Ô∏è (1:50:56) Adding and Removing Information
‚å®Ô∏è (1:58:30) Time Complexity Equations
‚å®Ô∏è (1:59:06) Uses of a Doubly-LinkedList
üíª (2:00:21) The Dictionary
‚å®Ô∏è (2:01:15) Dictionary Basics
‚å®Ô∏è (2:02:00) Indexing Dictionaries
‚å®Ô∏è (2:02:40) Dictionary Properties
üíª (2:05:53) Hash Table Mini-Lesson
‚å®Ô∏è (2:13:26) Time Complexity Equations
üíª (2:16:39) Trees
‚å®Ô∏è (2:16:55) Introduction to Hierarchical Data
‚å®Ô∏è (2:18:54) Formal Background on the Tree
‚å®Ô∏è (2:20:03) Tree Terminology and Visualization
‚å®Ô∏è (2:25:08) Different types of Trees
‚å®Ô∏è (2:28:07) Uses for the Tree
üíª (2:29:00) Tries
‚å®Ô∏è (2:29:50) Trie Basics
‚å®Ô∏è (2:30:41) Trie Visualization
‚å®Ô∏è (2:34:33) Flagging
‚å®Ô∏è (2:35:15) Uses for Tries
üíª (2:38:25) Heaps
‚å®Ô∏è (2:38:51) Heap Basics
‚å®Ô∏è (2:39:19) Min-Heaps
‚å®Ô∏è (2:40:07) Max-Heaps
‚å®Ô∏è (2:40:59) Building Heaps
‚å®Ô∏è (2:44:20) Deleting from Heaps
‚å®Ô∏è (2:46:00) Heap Implementations
üíª (2:48:15) Graphs
‚å®Ô∏è (2:49:25) Graph Basics
‚å®Ô∏è (2:52:04) Directed vs. Undirected Graphs
‚å®Ô∏è (2:53:45) Cyclic vs. Acyclic Graphs
‚å®Ô∏è (2:55:04) Weighted Graphs
‚å®Ô∏è (2:55:46) Types of Graphs
üíª (2:58:20) Conclusion