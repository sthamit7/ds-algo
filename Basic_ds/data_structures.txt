‚úçWhat are Data Structures?
- A way to store, organize & manage information(or data) in a way that allows you
the programmer to easily access or modify the values within them.
- Provide backbone to the programs
- Store information & access & manipulate information effectively
- Basic Data structures like => Password & online directories,etc
- Advanced Data structures Like => Autocomplete of text messages, undo, redo functions, etc
- Efficiency => the metrics used to judge the speed & Efficiency of different data structures
 
‚úçSeries Overview
üíª Arrays
üíª ArrayLists
üíª Stacks
üíª Queues
üíª LinkedLists
üíª Doubly-LinkedLists
üíª Dictionaries
üíª Hash-Tables
üíª Trees
üíª Tries
üíª Heaps
üíª Graphs

üíª (06:55) Measuring Efficiency with BigO Notation
- Quantifiable way to measure to how efficient certain data structures are at different tasks we might as of it 
 - Searching through
 - Modifying
 - Access
- The industry standard for this kind of measurement => BigO Notation
- BigO Notation => Based on four criteria
  - Accessing elements
  - Searching elements
  - Inserting elements
  - Deleting elements 
‚å®Ô∏è (09:45) Time Complexity Equations
   - Works by Inserting the size of the data-set as an integer n, and returning the number of operations that need to be conducted by the computer before the function can finish
   - The number of operations that need to be conducted by the computer before completion of that function
   - We always use the worst-case scenario when judging these data structures
‚å®Ô∏è (11:13) The Meaning of BigO
   - The syntax for the time complexity equations included a BigO and then a set of parenthesis
     - The parenthesis houses the function
     - A random function and its time complexity is measured based on the data sets, our integer n
     - n => number of operations
     - More data set => More instructions
‚å®Ô∏è (12:42) Why BigO?
     - We measure Efficiency in # of operations performed because measuring by how long the function takes to run would be silly => Measuring by time is biased towards better hardware
‚å®Ô∏è (13:18) Quick Recap
    - Measure Efficiency based on 4 metrics => Accessing, Searching, Inserting, Deleting
    - Modeled by an equation which takes in size of data-set n => number of operations needed to perform the task
‚å®Ô∏è (14:27) Types of Time Complexity Equations
    - O(1) =>Constant algorithm, absolute best, no matter what the size of your data set is , the task will be completed in a single instruction
    - O(log n) => Logarithmic algorithm, Next fastest type of time complexity, fast completion time, more efficient
    - O(n) => Linear algorithm Common time complexity, the last of the decent equations
    - O(n log n) => Relatively Bad
    - O(n2)=> Polynomial algorithm, Very bad in terms of Efficiency 
    - O(2n)=> Exponential algorithm, Very bad in terms of Efficiency
‚å®Ô∏è (19:42) Final Note on Time Complexity Equations
    - Time complexity equations are NOT the only metric you should be using the gauge which data structure to use 
    - Some provide other functionality that make them extremely useful
    
üíª (20:21) The Array
    - A pretty common data structure taught in most programming classes 

‚å®Ô∏è (20:58) Array Basics
    - fundamentally a list of similar values
    - can be store anything => usernames, high scores, prices, etc.
    - store values of the same data type => integer, float ,etc.
    - item referred to as "element"
    - three attributes => A name , A Type , A size 

‚å®Ô∏è (22:09) Array Names
    - Array Names => name of the array, used to reference & interact with it
      Example: Names =  ["John Smith", "Gary Vee"]; Salaries = [10000, 12400]

‚å®Ô∏è (22:59) Parallel Arrays
    - Two or more arrays which => contain the same no. of elements => have corresponding values in the same position
    - extremely useful for storing different types of data about the same entity

‚å®Ô∏è (23:59) Array Types
    - what type of information is stored or will be stored within that array
    - has to hold all the same type of information

‚å®Ô∏è (24:30) Array Size
    - Immutable 
    - a set integer that is fixed upon creation of the array
    - represents the total amount of elements that are able to be stored within the array

‚å®Ô∏è (25:45) Creating Arrays
    - 2 different ways to create an array in most languages
     => populate the array with elements right then & there
     => set a specific size for the array, then populate it later

‚å®Ô∏è (26:11) Populate-First Arrays
    - defining & filling an array as soon as you create it is used mainly for when you already know which values are going to be held within it
    - The way varies from languages => Java, Python, C#
     => int array[] = {1,2,3}; //Java 
     => array =[1,2,3]         // Python
     => int[] array = {1,2,3}; // C#

‚å®Ô∏è (28:09) Populate-Later Arrays
    - creating an array by setting an initial size for our array, but not filing it with any elements
     => slowly populate it as the programs run
     => used for user-entered information
     => int array [] =new int[10]; // Java
     => int[] arrray = new int[10]; // C#

‚å®Ô∏è (30:22) Numerical Indexes
    - get information that is stored withing the array, we use a number Indexes
    - an integer which corresponds to an element within the array

‚å®Ô∏è (31:57) Replacing information in an Array
    - Referencing an arrays index is also h0w we replace elements within an array

‚å®Ô∏è (32:42) 2-Dimensional Arrays
    - an array with an array at each index is known as a 2-Dimensional array
    - useful for programming chessboard, bingo board, image with rgb values,etc
    - includes 2 indexes => one for the column and other with row
    - there are 3d arrays and 4d arrays for complex programming

‚å®Ô∏è (35:01) Arrays as a Data Structure
    - accessing, searching, inserting, deleting in an array
    - accessing => O(1) that is why arrays are fixed size
    - searching => O(n) because of unsorted lists, must use linear search
    - inserting => O(n) because inserting an element within the array requires you to shift every element that's after the index you want to insert the value at to the right one space
    - deleting => O(n) because deleting an element within the array requires you to shift every element to the right of the one you want to delete down one index

‚å®Ô∏è (42:21) Pros and Cons
    - Pros => Good for storing similar contiguous data, O(1) accessing power, very basic, easy to learn and master
    - Cons => size of the array cannot be changed once Initialized,
    inserting and deleting are not efficient, can be wasting storage space
    - Overall, pretty reliable, has some flaws as well as advantages, Can be used in almost any program if need be, but sometimes you may want extra functionality

üíª (43:33) The ArrayList
    - can be thought as growing array
    - size is dynamic

‚å®Ô∏è (44:42) Structure of the ArrayList
    - backed by an array
    - makes it have a similar functionality

‚å®Ô∏è (45:19) Initializing an ArrayList
    - varies based on languages
    - Python list are both array and ArrayLists
    - ArrayList<integer> arrayList = new ArrayList<integer>(); //Java
    - ArrayList arrayList = new ArrayList(); //C#

‚å®Ô∏è (47:34) ArrayList Functionality
    - can be thought as evolved array
    - beefier, more functionality, more powerful
    - pre-build arrayList "class" => prebuilt functions that are at our disposal
    - varies based on languages

‚å®Ô∏è (49:30) ArrayList Methods
    - because of the variability, only 6 common methods
    
‚å®Ô∏è (50:26) Add Method
    - two different types: 
    - Add(Object)=> appends the element you pass in as an argument to the end of the arrayList => Autoboxing, puts the object to next open indexes
    - Add(Object, Index) => appends the element you pass in as an argument at the index you pass in

‚å®Ô∏è (53:57) Remove Method
    - two methods:
    - Remove(Index) => removes the object at the index you provide
    - Remove(Object) => removes the first instance of the object passed into the arrayList

‚å®Ô∏è (55:33) Get Method
    - Get(Index) => returns the object contained at the given index

‚å®Ô∏è (55:59) Set Method
    - set method is how we replace elements within the arrayList
    - Set (Index, Object) => sets the element at the index which you passed in, to the object you also passed in

‚å®Ô∏è (56:57) Clear Method
    - Clear() => clears the arrayList, deleting every element entirely

‚å®Ô∏è (57:30) toArray Method
    - used to convert an arrayList to an Array
    - toArray() => converts the arrayList to an array. Must be equal to the creation of an new array

‚å®Ô∏è (59:00) ArrayList as a Data Structure
    - accessing, searching, inserting, deleting
    - accessing => get method, O(1), 
    - searching => O(n)
    - inserting => Add(Object,Index); Add(Object), O(n)
    - deleting => Remove(Object),Remove(Index),O(n)

‚å®Ô∏è (1:03:12) Comparing and Contrasting with Arrays
    - Arrays- Fixed size, can store all data types, methods to be created,
    don't requires much memory and upkeep,
      smaller tasks, where you won't be interacting or changing the data
    - ArrayLists- Dynamic size, can only store objects, methods are created for you, requires more memory use and upkeep
      more interactive programs where you will be Modifying the data

üíª (1:05:02) The Stack
‚å®Ô∏è (1:05:06) The Different types of Data Structures
‚å®Ô∏è (1:05:51) Random Access Data Structures
    - Provides O(1) Accessing
    - Independent elements
    - Arrays and ArrayLists

‚å®Ô∏è (1:06:10) Sequential Access Data Structures
    - can only be accessed in a particular order
    - Each element is dependent on the others
    - May only be obtainable through those those other elements
    - also limited access 
    - Do not provide O(1) accessing
    - Stacks, Queues, Linked lists

‚å®Ô∏è (1:07:36) Stack Basics
    - We add elements and remove elements according to the LIFO principle
    - Last in First Out

‚å®Ô∏è (1:09:01) Common Stack Methods
    - Push Method
    - Pop Method
    - Peek Method
    - Contains Method

‚å®Ô∏è (1:09:45) Push Method
    - Pushes an object or element onto the group of top of the Stack
    - Push(object)
    - exampleStach.push("something")

‚å®Ô∏è (1:10:32) Pop Method
    - removes an element from the top of the Stack
    - Pop()

‚å®Ô∏è (1:11:46) Peek Method
    - Allows you to get the value at the top of the list without removing it
    - Peek()

‚å®Ô∏è (1:12:27) Contains Method
    - Used for searching through the Stack
    - contains()

‚å®Ô∏è (1:13:23) Time Complexity Equations
    - accessing: O(n)
    - searching: O(n)
    - inserting: O(1), push(object)
    - deleting: O(1) pop()

‚å®Ô∏è (1:15:28) Uses for Stacks
    - are used everywhere, both in the actual writing of code as well as in the real-world situations
    - Recursion=> The process of functions repeatedly calling themselves
    - Undo/Redo, Back-programming

üíª (1:18:01) The Queue
    - Sequential access data structure which follows the FIFO methodology
‚å®Ô∏è (1:18:51) Queue Basics
    - Add elements in tail
    - Remove elements from Head

‚å®Ô∏è (1:20:44) Common Queue Methods
    - Enqueue Method
    - Dequeue Method
    - Peek Method
    - Contains Method

‚å®Ô∏è (1:21:13) Enqueue Method
    - Enqueue(object)
    - example.enqueue("something")

‚å®Ô∏è (1:22:20) Dequeue Method
    - Dequeue(object)
    - example.dequeue("something")

‚å®Ô∏è (1:23:08) Peek Method
    - returns the object that's at the forefront of our Queue
    - Peek()

‚å®Ô∏è (1:24:15) Contains Method
    - returns whether or not the queue contains an object

‚å®Ô∏è (1:25:05) Time Complexity Equations
    - accessing: O(n)
    - searching: O(n)
    - inserting: O(1)
    - deleting: O(1)

‚å®Ô∏è (1:27:05) Common Queue Uses
    - used very often in programming variety of functions
    - job scheduling, printer queueing,
    - modern cameras

üíª (1:28:16) The Linked List
    - Sequential access
    - linear data structure
    - every element is a separate object called a node, which has 2 parenthesis
    - data, reference => pointer which points to the next node in the List

‚å®Ô∏è (1:31:37) LinkedList Visualization
    - the list can have null if it is the only node
    - the head node points to another node
    - the tail node points to null

‚å®Ô∏è (1:33:55) Adding and Removing Information
    - data can flow in and out of any point of a linked list
    - add or remove from head, middle or tail
    - add to head => make that new node's pointer to the current head of the linked list
    - remove from head => value as null
    - add to middle => make the pointer of new node point to the node after the location we want to insert at, set the node before the location we want to insert at to point towards the new node
    - remove from middle => make the pointer of the node previous to the one we're removing, to now point to the node after the one we're removing
    - add to tail => make the current tail point towards the new node you want to add
    - remove from tail => set the previous tail to point towards a null value instead of the current tail

‚å®Ô∏è (1:41:28) Time Complexity Equations
    - accessing: O(n)
    - searching: O(n)
    - inserting: O(n) or  O(1)
    - deleting: O(n) or  O(1)

‚å®Ô∏è (1:44:26) Uses for LinkedLists
    - can be used in the backing of other data structures
    - we can use linkedlists to make Stacks, Queues, etc.
    - music player playlist
    - photo gallery
    Drawback => only go forward, never backward

üíª (1:47:19) The Doubly-LinkedList
    - same as linked list
    - able to traverse both forwards and backwards using pointers
    - previous pointer and next pointer

‚å®Ô∏è (1:48:44) Visualization
    - Head  node can have with null previous and next pointers
 
‚å®Ô∏è (1:50:56) Adding and Removing Information
    - Add to head=> set the new nodes next to point towards the current head of the list,
    take the new node that we want to insert, and set its previous to null, set the current head's previous to point towards this new node
    - Remove from head=> set hte head nodes next to point towards a null value, set the second nodes previous to also point towards a null value. 
    - Add to middle=> set the new nodes' previous to point towards the node previous to the position you want to insert at, set the new nodes next to point towards the node after the position you want to insert at, set the next and previous on the node's before and after the one youre inserting to point towards the new node
    - Remove from middle=> set the node before the one you want to remove's next to point towards the node after the one we want to remove, set the node after the one we want to remove's previous to point towards the node before the one we want to remove, set both pointers of node we want to remove to a null value
    - Add to tail=> set the next pointer of the current tail to point towards the new node we want to become the tail, set the previous of the new node that we're adding to be pointing towards the current tail of the list, make the new nodes next point towards a null value
    - Remove from tail => set the tail nodes previous to point towards null, set the second to last node's next to also points towards the null.

‚å®Ô∏è (1:58:30) Time Complexity Equations
    - accessing: O(n)
    - searching: O(n)
    - inserting: O(n) or  O(1)
    - deleting: O(n) or  O(1)

‚å®Ô∏è (1:59:06) Uses of a Doubly-LinkedList
    - the back and forth functionality
    - webpages switching functions, undo-redo functions, open recent functions

üíª (2:00:21) The Dictionary
    - one of the most abstract data structures
    - also called maps and associative arrays
    - key/value pairs 

‚å®Ô∏è (2:01:15) Dictionary Basics
    - think of key/value pair like a social security number

‚å®Ô∏è (2:02:00) Indexing Dictionaries
    - we index Dictionaries using these keys instead of numberical index

‚å®Ô∏è (2:02:40) Dictionary Properties
    - the key's in key/value pair can be any primitive dat type
    - strings, another Dictionaries, boolean, integers,
    2 extremely important restrictions
    - every key can only appear once within the Dictionary
    - each key can only have one value
    - there can be duplicates of values within a Dictionary

üíª (2:05:53) Dictionary Hash Table Mini-Lesson
    - cannot index the key, because its based upon the simple assumption that the size of the array is not too large
    - fundamentally a way to store this information in such a way that we're able to cut down the amount of nil values while also allowing for the information to be stored in a way that is easily accessible.
    - Hash function => will take all the keys for a given Dictionary and strategically map them to certain index locations in an array so that they can eventually be retrieved easily
    - The goal of a good hashing function is to take in a key an reliably place it somewhere in the table so that it can be accessed later by the computer

    - Take the key, divide it by itself, then multiply the result by the number of digits in the key minus 1 => (1000000/1000000) x (7-1) 1million is the key, this key will be in index 6. 
    - we can consolidate the keys from many keys into index slots
    - Dictionaries are built upton these hash tables, and the key's in out key/value pairs are stored in memory in these hash tables at indexes which are determined by a hash function

    -What happens when we run two different dictionary keys in to a has function, and the computer tells us to store them at the same index location?
    - This can occur Hash collision
    - can be solved 2 ways => Open addressing & Closed addressing
    - open addressing => put the key in some other index location separate from the one returned to us by the hash function
    - closed addressing => uses linked list to chain together keys which result in the same has value

‚å®Ô∏è (2:13:26) Time Complexity Equations
    Worst case scenario
    - accessing: O(n)
    - searching: O(n)
    - inserting: O(n) or  O(1)
    - deleting: O(n) or  O(1)
    Average case scenario
    - accessing: O(1)
    - searching: O(1)
    - inserting: O(1)
    - deleting: O(1)

    Uses: 
    - the option for non-numerical indexes
    - the flexibility when it comes to making keys
    - the speed which comes with the hash table Implementation

üíª (2:16:39) Trees
‚å®Ô∏è (2:16:55) Introduction to Hierarchical Data
    - store data Hierarchically as opposed to linearly
    - File structure in the computer can be a good example
 
‚å®Ô∏è (2:18:54) Formal Background on the Tree
    - abstract data structure which contains a series of linked nodes connected together to form a Hierarchical representation of information
    - like a linkedlists where each node has the option of pointing towards a multiple nodes

‚å®Ô∏è (2:20:03) Tree Terminology and Visualization
    - Vertice => A certain node in a Tree
    - Edge => A connection between nodes
    - Root node => Topmost node of a Tree
    - Child node => A certain node which has an edge connecting it to another node one level above itself
    - parent node => any node which has 1 or more child nodes
    - leaf node => a node in a tree which does not have any child nodes
    - Height => Property of the tree => number of edges on the longest possible path down towards a leaf
    - depth => property of a node => Number of edges required to get from that particular node to the root node

‚å®Ô∏è (2:25:08) Different types of Trees
    - regular trees => great for storing Hierarchical data, but their power can really be Heightened when you start messing around with how the data is actually stored within them
    => Tries, Graphs, Heaps
    => Binary Search tree => simple variation on the standard tree which has three restrictions on it to help organize the data
        - a node can have at most 2 children
        - for any give parent node, the child to the left has a value less than or equal to itself, and the right has a value greater than or equal to itself
        - no 2 nodes can contain the same value
        Advantages: 
        - we are able to search through them in Logarithmic time
        
‚å®Ô∏è (2:28:07) Uses for the Tree
    - data sets => file structure, family tree, company Hierarchy, etc

üíª (2:29:00) Tries
    - Trees can become even more useful once you start setting restrictions on how and where data can be stored within them
    - often overlooked since it is only used in specific situations

‚å®Ô∏è (2:29:50) Trie Basics
    - is a tree-like data structure whose nodes store letters of an alphabet in the form of characters
    - we can carefully construct this tree of characters in such a ay which allows us to quickly retrieve words in the form of strings by traversing down a path of the trie in a certain way

‚å®Ô∏è (2:30:41) Trie Visualization
    - root node can be empty with references
    - similarly with every node can have characters with references and traverse

‚å®Ô∏è (2:34:33) Flagging
    - marking the end of a word by having it also point towards a "flag" to let the computer know that the end of a word has occurred
    
‚å®Ô∏è (2:35:15) Uses for Tries
    - limited but effective => autofilling, spell check, etc.
    - big programs like google docs dont just use tries containing a few words or even all words starting with a certain letter. they are storing entire english dictionary
    
üíª (2:38:25) Heaps
    - similar to tree

‚å®Ô∏è (2:38:51) Heap Basics
    - special tree where all parent nodes compare to their children node's in some specific way by being more or less extremely
        - Either greater than or less than
        - Determines where the data is stored
        - Usually dependent on the parent Node's value

‚å®Ô∏è (2:39:19) Min-Heaps
    - the value at the root node must be minimum amongst all of its children
    - this fact must be the same recursively for all parent node's contained within the heap

‚å®Ô∏è (2:40:07) Max-Heaps
    - the value at the root node must be maximum amongst all of its children
    - this fact must be the same recursively for all parent node's contained within the heap

‚å®Ô∏è (2:40:59) Building Heaps

‚å®Ô∏è (2:44:20) Deleting from Heaps
‚å®Ô∏è (2:46:00) Heap Implementations
üíª (2:48:15) Graphs
‚å®Ô∏è (2:49:25) Graph Basics
‚å®Ô∏è (2:52:04) Directed vs. Undirected Graphs
‚å®Ô∏è (2:53:45) Cyclic vs. Acyclic Graphs
‚å®Ô∏è (2:55:04) Weighted Graphs
‚å®Ô∏è (2:55:46) Types of Graphs
üíª (2:58:20) Conclusion